{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Netflix](https://miro.medium.com/max/1400/1*jlQxemlP9Yim_rWTqlCFDQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Amazon](images/amazon_recommender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- describe the difference between content-based and collaborative-filtering algorithms\n",
    "- explain and use the cosine similarity metric\n",
    "- describe the algorithm of alternating least-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNFORTUNATELY the `surprise` library might be broken in y'all's environments. If so, use these two commands in a new terminal window, then reset the kernel:\n",
    "\n",
    "```\n",
    "pip uninstall scikit-surprise\n",
    "conda install -c conda-forge scikit-surprise\n",
    "```\n",
    "\n",
    "NOTE! CHECK THE TERMINAL WINDOW OUTPUT! Do NOT let it upgrade to numpy version 1.20!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender systems can be classified along various lines. One fundamental distinction is **content-based** vs. **collaborative-filtering** systems.\n",
    "\n",
    "To illustrate this, consider two different strategies: (a) I recommend items to you that are *similar to other items* you've used/bought/read/watched; and (b) I recommend items to you that people *similar to you* have used/bought/read/watched. The first is the **content-based strategy**; the second is the **collaborative-filtering strategy**. \n",
    "\n",
    "Another distinction drawn is in whether (a) the system uses existing ratings to compute user-user or item-item similarity, or (b) the system uses machine learning techniques to make predictions. Recommenders of the first sort are called **memory-based**; recommenders of the second sort are called **model-based**.\n",
    "\n",
    "One last distinction:  **explicit** versus **implicit** ratings.\n",
    "\n",
    "- **_Explicit_** data is gathered from users when we ask a user to rate an item on some scale\n",
    "    - Pros: concrete rating system, can assume users actually feel the way they input and thus can extrapolate from those preferences\n",
    "    - Cons: not all users might input their preferences\n",
    "- **_Implicit_** data is gathered from users without their direct input - a system logs the actions of a user\n",
    "    - Pros: Easier to collect automatically, thus have more data from more users without those users needing to go through extra steps\n",
    "    - Cons: More difficult to work with - how do we know what actions imply preference?\n",
    "    \n",
    "[insert comment about y'all filling out surveys here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First - The Dumb Model-Less (and Memory-Less) Approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Personalized Recommendations\n",
    "\n",
    "<img src=\"images/youtube-nonpersonalizedrecommendations.png\" alt=\"screenshot of youtube's homepage\" width=600>\n",
    "\n",
    "YouTube is notorious for putting non-personalized content on their homepage (although they tailor recommendations in other places)\n",
    "\n",
    "These recommendations are based purely on the popularity of the item!\n",
    "\n",
    "#### Advantages\n",
    "- Super easy (computationally and for the user to understand)\n",
    "- Items are usually popular for a reason\n",
    "- No cold-start issue\n",
    "\n",
    "#### Disadvantages\n",
    "- Not personalized\n",
    "- New items wonâ€™t gain traction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea here is to recommend items to a user that are *similar to* items that the user has already enjoyed. Suppose we represent TV shows as rows, where the columns represent various features of these TV shows. These features might be things like the presence of a certain actor or the show fitting into a particular genre etc. We'll just use binary features here, perhaps the result of some one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_shows = np.array([[0, 1, 1, 0, 1, 1, 1],\n",
    "                    [0, 0, 0, 1, 1, 1, 0],\n",
    "                    [1, 1, 1, 0, 0, 1, 1],\n",
    "                    [0, 1, 1, 1, 0, 0, 1]])\n",
    "\n",
    "tv_shows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob likes the TV Show represented by Row \\#1. Which show (row) should we recommend to Bob?\n",
    "\n",
    "One natural way of measuring the similarity between two vectors is by the **cosine of the angle between them**. Two points near one another in feature space will correspond to vectors that nearly overlap, i.e. vectors that describe a small angle $\\theta$. And as $\\theta$ decreases, $\\cos(\\theta)$ *increases*. So we'll be looking for large values of the cosine (which ranges between -1 and 1). We can also think of the cosine between two vectors as the *projection of one vector onto the other*:\n",
    "\n",
    "![image.png](https://www.oreilly.com/library/view/statistics-for-machine/9781788295758/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png)\n",
    "\n",
    "We can use this metric easily if we treat our rows (the items we're comparing for similarity) as vectors: We can calculate the cosine of the angle $\\theta$ between two vectors $\\vec{a}$ and $\\vec{b}$ as follows: $\\cos(\\theta) = \\frac{\\vec{a}\\cdot\\vec{b}}{|\\vec{a}||\\vec{b}|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerators = np.array([tv_shows[0].dot(tv_show) for tv_show in tv_shows[1:]])\n",
    "denominators = np.array([np.sqrt(sum(tv_shows[0]**2)) *\\\n",
    "                         np.sqrt(sum(tv_show**2)) for tv_show in tv_shows[1:]])\n",
    "\n",
    "numerators / denominators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the cosine similarity to Row \\#1 is highest for Row \\#3, we would recommend this TV show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Metrics\n",
    "\n",
    "What items are 'similar'? Depends on your similarity metric:\n",
    "\n",
    "![similarity metrics comparison](images/similaritymetrics.png)\n",
    "\n",
    "[Image Source: \"What Similarity Metric Should You Use for Your Recommendation System?](https://medium.com/bag-of-words/what-similarity-metric-should-you-use-for-your-recommendation-system-b45eb7e6ebd0) <- useful reading!\n",
    "\n",
    "Those are just 3 examples, there are others (Jaccard index, Euclidian similarity) - but the point is you take some mathematical understanding of the items and find which ones are 'nearby' in some sense.\n",
    "\n",
    "#### Advantages:\n",
    "- Easy and transparent\n",
    "- No cold start issue\n",
    "- Recommend items to users with unique tastes\n",
    "\n",
    "#### Disadvantages:\n",
    "- Requires some type of tagging of items\n",
    "- Overspecialization to certain types of items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "\n",
    "Now the idea is to recommend items to a user based on what *similar* users have enjoyed. \n",
    "\n",
    "Suppose we have the following recording of explicit ratings of five items by three users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = np.array([[5, 4, 3, 4, 5], [3, 1, 1, 2, 5], [4, 2, 3, 1, 4]])\n",
    "\n",
    "new_user = np.array([5, 0, 0, 0, 0])\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To which user is `new_user` most similar?\n",
    "\n",
    "One metric is cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user_mag = 5\n",
    "\n",
    "numerators = np.array([new_user.dot(user) for user in users])\n",
    "denominators = np.array([new_user_mag * np.sqrt(sum(user**2))\\\n",
    "                         for user in users])\n",
    "\n",
    "numerators / denominators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we could also use another metric, such as Pearson Correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.corrcoef(new_user, user)[0, 1] for user in users]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages:\n",
    "\n",
    "- Finds patterns among users\n",
    "- Capable of accurately recommending complex items without requiring any 'understanding' of the items themselves\n",
    "\n",
    "#### Disadvantages:\n",
    "\n",
    "- Cold start issue - need user behavior or ratings to determine who's similar\n",
    "- Hard to tell when users will be similar and when they won't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on content-based vs. collaborative systems, see [this Wikipedia article](https://en.wikipedia.org/wiki/Collaborative_filtering) and [this blog post](https://towardsdatascience.com/recommendation-systems-models-and-evaluation-84944a84fb8e). [This post](https://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/) on dataconomy is also useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "Use past behavior of many users (how they've rated many items) to find similarities based on both users and items.\n",
    "\n",
    "Suppose we start with a matrix $R$ of users and items, where each cell records the ranking the relevant user gave to the relevant item. Very often we'll be able to record this data as a sparse matrix, because many users will not have ranked many items.\n",
    "\n",
    "Then, we use **_MATH_** (namely, matrix factorization) to fill in those blanks, based upon similar users' ratings of similar items.\n",
    "\n",
    "More specifically, it finds factor matrices which result in the ratings it has - decomposing the actual Utility Matrix into component pieces that explain it. These component pieces, matrices themselves, can be thought of as 'latent' or 'inherent' features of the items and users! The math then comes in, as we calculate the dot products in order to arrive at our predicted ratings.'\n",
    "\n",
    "<img src=\"images/matrixfactorization.png\" alt=\"matrix factorization image, showing the factor matrices\" width=700>\n",
    "\n",
    "[Image Source](https://medium.com/@connectwithghosh/simple-matrix-factorization-example-on-the-movielens-dataset-using-pyspark-9b7e3f567536)\n",
    "\n",
    "A bit more on Matrix Factorization, from Google's Recommendations Systems crash course: https://developers.google.com/machine-learning/recommendation/collaborative/matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages:\n",
    "- Personalized. Youâ€™re special!\n",
    "\n",
    "#### Disadvantages:\n",
    "- Can require a lot of computation, especially as these matrices get larger\n",
    "- Cold start: need to have a lot of ratings to be worthwhile\n",
    "- Popularity Bias: biased towards items that are popular. May not capture peopleâ€™s unique tastes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix factorization methods include Singular Value Decomposition (SVD) and Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now, in code!\n",
    "\n",
    "### Reading in the data and simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the documentation for the Surprise library [here](https://surprise.readthedocs.io/en/stable/getting_started.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = surprise.Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've downloaded the data, we can find it in a hidden directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/.surprise_data/ml-100k/ml-100k/u.data',\n",
    "                 sep='\\t', header=None)\n",
    "df = df.rename(columns={0: 'userID', 1: 'movieID', 2: 'rating', 3: 'timestamp'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check value_counts\n",
    "ratings = df['rating'].value_counts()\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sorted = dict(zip(ratings.index, ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution in matplotlib\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(ratings_sorted.keys(), ratings_sorted.values(), width=.4)\n",
    "plt.xticks(np.arange(0, 5.1, step=0.5))\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"# of Ratings\")\n",
    "plt.title(\"Distribution of Ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of users: \", df.userID.nunique()) \n",
    "print(\"Average Number of Reviews per User: \", df.shape[0]/df.userID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_user = df['userID'].value_counts()\n",
    "ratings_per_user = sorted(list(zip(ratings_per_user.index, ratings_per_user)))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar([r[0] for r in ratings_per_user], [r[1] for r in ratings_per_user])\n",
    "plt.xlabel(\"User ID\")\n",
    "plt.ylabel(\"# of Reviews\")\n",
    "plt.title(\"Number of Reviews per User\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of movies: \", df.movieID.nunique())\n",
    "print(\"Average Number of Reviews per Movie: \", df.shape[0]/df.movieID.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the movie IDs with the most ratings\n",
    "df['movieID'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_per_movie = df['movieID'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(ratings_per_movie, bins=50)\n",
    "plt.xlabel(\"# of Reviews\")\n",
    "plt.ylabel(\"# of Movies\")\n",
    "plt.title(\"Distribution of the Number of Ratings Per Movie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition using Surprise\n",
    "\n",
    "Written by Yish, thanks <3\n",
    "\n",
    "One of the easiest libraries to use for recommendation systems is Surprise, which stands for **Simple Python Recommendation System Engine**. Here, we'll code a recommendation system using the Surprise Library's Singular Value Decomposition (SVD) algorithm.\n",
    "\n",
    "To read more about Surprise's SVD implementation, and its hyperparameters:\n",
    "https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test-split - using surprise!\n",
    "train, test = surprise.model_selection.train_test_split(data, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate SVD from surprise\n",
    "svd = None # use default values\n",
    "\n",
    "# now fit to train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `test` method of the model on the test set to get out predictions\n",
    "predictions = None\n",
    "# Now check the RMSE of those predictions!\n",
    "surprise.accuracy.rmse(predictions);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the MAE\n",
    "surprise.accuracy.mae(predictions);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at the first 10 rows of our test set\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of users: \", df.userID.nunique()) \n",
    "print(\"Number of movies: \", df.movieID.nunique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict! Can play around with the users & items\n",
    "user = 5\n",
    "item = 141\n",
    "svd.predict(user, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Models? More Models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprise has some basic algorithms - like `BaselineOnly`, which predicts a baseline estimate for a given user an item.\n",
    "\n",
    "https://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.baseline_only.BaselineOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = surprise.BaselineOnly()\n",
    "base.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise.accuracy.mae(base.test(test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise.accuracy.rmse(base.test(test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "Plus there are always neighbors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = surprise.KNNBasic()\n",
    "knn.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise.accuracy.mae(knn.test(test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise.accuracy.rmse(knn.test(test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# find the nearest neighbors to an item\n",
    "knn.get_neighbors(iid=item, k=1) # using same item as earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to try more? Let's explore!\n",
    "\n",
    "https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here to explore other recommendation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALS in `pyspark`\n",
    "\n",
    "We'll talk about Big Data and Spark soon (next week!), but I'll just note here that Spark has a recommendation submodule inside its ml (machine learning) module. Source code for `pyspark`'s version [here](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
