{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Pre-Processing & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to install nltk if needed\n",
    "# !pip install nltk\n",
    "# !conda install -c anaconda nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Notice that these vectorizers are from `sklearn` and not `nltk`!\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,\\\n",
    "HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use this to download the stopwords if you haven't already - only ever needs to be run once\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same with wordnet!\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And with parts of speech tagging!\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "- Describe the basic concepts of NLP\n",
    "- Use pre-processing methods for NLP\n",
    "    - Tokenization\n",
    "    - Stopwords removal\n",
    "- normalize a lexicon with stemming and lemmatization\n",
    "- run feature engineering algorithms for NLP\n",
    "    - bag-of-Words\n",
    "    - vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of NLP\n",
    "\n",
    "NLP allows computers to interact with text data in a structured and sensible way. In short, we will be breaking up series of texts into individual words (or groups of words), and isolating the words with **semantic value**.  We will then compare texts with similar distributions of these words, and group them together.\n",
    "\n",
    "In this section, we will discuss some steps and approaches to common text data analytic procedures. Some of the applications of natural language processing are:\n",
    "- Chatbots \n",
    "- Speech recognition and audio processing \n",
    "- Classifying documents \n",
    "\n",
    "Here is an example that uses some of the tools we use in this notebook.  \n",
    "  -[chicago_justice classifier](https://github.com/chicago-justice-project/article-tagging/blob/master/lib/notebooks/bag-of-words-count-stemmed-binary.ipynb)\n",
    "\n",
    "We will introduce you to the preprocessing steps, feature engineering, and other steps you need to take in order to format text data for machine learning tasks. \n",
    "\n",
    "We will also introduce you to [**NLTK**](https://www.nltk.org/) (Natural Language Toolkit), which will be our main tool for engaging with textual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nlp_process.png\" style=\"width:1000px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal when pre-processing text data for NLP is to remove as many unnecessary words as possible while preserving as much semantic meaning as possible. This will improve your model performance dramatically.\n",
    "\n",
    "You can think of this sort of like dimensionality reduction. The unique words in your corpus form a **vocabulary**, and each word in your vocabulary is essentially another feature in your model. So we want to get rid of unnecessary words and consolidate words that have similar meanings.\n",
    "\n",
    "We will be working with a dataset which includes both satirical (The Onion) and real news (Reuters) articles. We refer to the entire set of articles as the **corpus**.  \n",
    "\n",
    "Each article in the corpus is refered to as a **document**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![the_onion](images/the_onion.jpeg) ![reuters](images/reuters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data, check it out\n",
    "corpus = pd.read_csv('data/satire_nosatire.csv')\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the data look like\n",
    "corpus.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to detect satire, so our target class of 1 is associated with The Onion articles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out a satirical article\n",
    "corpus.loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout out a non-satirical article\n",
    "corpus.loc[502]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a balanced dataset with 500 documents of each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Let's think about the use cases of being able to correctly separate satirical from authentic news. What might be a real-world use case?  \n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Processing By Hand!\n",
    "\n",
    "Let's go over the many steps involved in processing text data. While some functions or classes we'll use later will do many of these steps for us, we'll walk through each of these manually first so we can discuss why they're important!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "\n",
    "In order to convert the texts into data suitable for machine learning, we need to break down the documents into smaller parts. \n",
    "\n",
    "The first step in doing that is **tokenization**.\n",
    "\n",
    "Tokenization is the process of splitting documents into units of observations. We usually represent the tokens as __n-grams__, where n represent the number of consecutive words occuring in a document that we will consider a unit. In the case of unigrams (one-word tokens), the sentence \"David works here\" would be tokenized into:\n",
    "\n",
    "- \"David\", \"works\", \"here\";\n",
    "\n",
    "If we want (also) to consider bigrams, we would (also) consider:\n",
    "\n",
    "- \"David works\" and \"works here\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a particular document in our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document = corpus.iloc[1].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to tokenize our document. \n",
    "\n",
    "It is a long string, so the first way we might consider is to split it by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here to split it by spaces\n",
    "tokens = None\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is not ideal. We are trying to create a set of tokens with **high semantic value**.  In other words, we want to isolate text which best represents the meaning in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "\n",
    "Most NL Pre-Processing will include the following tasks:\n",
    "\n",
    "  1. Remove capitalization  \n",
    "  2. Remove punctuation  \n",
    "  3. Remove stopwords  \n",
    "  4. Remove numerals\n",
    "  \n",
    "We can manually perform all of these tasks with string operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization\n",
    "\n",
    "When we create our matrix of words associated with our corpus, **capital letters** will mess things up.  The semantic value of a word used at the beginning of a sentence is the same as that same word in the middle of the sentence - but Python won't treat them the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will prove that 'excessive' and 'Excessive' aren't the same\n",
    "sentence_one =  \"Excessive gerrymandering in small counties suppresses turnout.\" \n",
    "sentence_two =  \"Turnout is suppressed in small counties by excessive gerrymandering.\"\n",
    "\n",
    "Excessive = sentence_one.split(' ')[0]\n",
    "excessive = sentence_two.split(' ')[-2]\n",
    "print(excessive, Excessive)\n",
    "excessive == Excessive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While 'excessive' has the same semantic value, it will be treated as different tokens because of capitals. Let's fix it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code here to remove capital letters and make everything lowercase\n",
    "# Let's do this for our tokens\n",
    "manual_cleanup = None\n",
    "manual_cleanup[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the length of the original number of tokens in sample_document\n",
    "print(f\"Our initial token set for our sample document is {len(tokens)} words long\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the unique number of tokens in sample_document\n",
    "print(f\"Our initial token set for our sample document has {len(set(tokens))} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the length of the unique number of tokens after lowercasing\n",
    "print(f\"After removing capitals, our sample document has {len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So - it didn't change the number of unique words in our sample document. Part of the reason why is because it's still dealing with punctuation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "\n",
    "Like capitals, splitting on white space will create tokens which include punctuation that will muck up our semantics.  \n",
    "\n",
    "Returning to the above example, 'gerrymandering' and 'gerrymandering.' will be treated as different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out how the same word with and without punctuation aren't the same\n",
    "no_punct = sentence_one.split(' ')[1]\n",
    "punct = sentence_two.split(' ')[-1]\n",
    "print(no_punct, punct)\n",
    "no_punct == punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can access punctuation using the string library\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun way to clean this is using the translate function!\n",
    "manual_cleanup = [s.translate(str.maketrans('', '', string.punctuation))\\\n",
    "                  for s in manual_cleanup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"After removing punctuation, our sample document has \\\n",
    "{len(set(manual_cleanup))} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in our sample document, we now have 3 fewer unique tokens! Still more work to do though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are the **filler** words in a language: prepositions, articles, conjunctions. They have low semantic value, and often need to be removed.  \n",
    "\n",
    "Luckily, NLTK has lists of stopwords ready for our use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out some English language stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK has stopwords in other languages too!\n",
    "stopwords.words('greek')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which stopwords are present in our sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the first 10 stopwords in our sample document\n",
    "stops = [token for token in manual_cleanup if token in stopwords.words('english')]\n",
    "stops[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(stops)} instances of {len(set(stops))} \\\n",
    "stopwords in the sample document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The {len(stops)} instances make up \\\n",
    "{len(stops)/len(manual_cleanup): 0.2%} of our text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also use the **FreqDist** tool to look at the makeup of our text before and after removal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This FreqDist class from NLTK will help us visualize the frequency of words\n",
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's remove stopwords - use list comprehension\n",
    "sw = stopwords.words('english')\n",
    "manual_cleanup = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After removing stopwords, there are {len(set(manual_cleanup))} unique words left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check out that frequency distribution\n",
    "fdist = FreqDist(manual_cleanup)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerals\n",
    "\n",
    "Numerals also usually have low semantic value. Their removal can help improve our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_cleanup = [s.translate(str.maketrans('', '', '0123456789')) \\\n",
    "                  for s in manual_cleanup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop empty strings\n",
    "manual_cleanup = [s for s in manual_cleanup if s != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'After removing numerals, there are {len(set(manual_cleanup))} unique words left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex\n",
    "\n",
    "Regex allows us to match strings based on a pattern.  This pattern comes from a language of identifiers, which we can begin exploring on the cheatsheet found here:\n",
    "  -   https://regexr.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few key symbols:\n",
    "  - . : matches any character\n",
    "  - \\d, \\w, \\s : represent digit, word, whitespace  \n",
    "  - *, ?, +: matches 0 or more, 0 or 1, 1 or more of the preceding character  \n",
    "  - [A-Z]: matches any capital letter  \n",
    "  - [a-z]: matches lowercase letter  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other helpful resources:\n",
    "  - https://regexcrossword.com/\n",
    "  - https://www.regular-expressions.info/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use regex to isolate numerals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[0-9]'\n",
    "number = re.findall(pattern, sample_document)\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern2 = '[0-9]+'\n",
    "number2 = re.findall(pattern2, sample_document)\n",
    "number2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Using NLTK\n",
    "\n",
    "## `RegexpTokenizer()`\n",
    "\n",
    "SKLearn and NLTK provide us with a suite of **tokenizers** for our text preprocessing convenience. So we don't have to do this all by hand every time!\n",
    "\n",
    "Let's use a provided Regex pattern to do all that work for us!\n",
    "\n",
    "Documentation: https://tedboy.github.io/nlps/generated/generated/nltk.tokenize.RegexpTokenizer.html\n",
    "\n",
    "\n",
    "Can use [RegExr](https://regexr.com/) to explore what our pattern, `([a-zA-Z]+(?:'[a-z]+)?)`, is doing exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pattern tokenizes AND removes punctuation\n",
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "# Need to save the tokenizer that uses the regex pattern\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "# And now tokenize our sample_document with tokenizer.tokenize\n",
    "sample_tokens = tokenizer.tokenize(sample_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out one of the middle sentences - notice what happened to I'm\n",
    "sample_tokens[42:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to lowercase\n",
    "sample_tokens = [token.lower() for token in sample_tokens]\n",
    "# And remove stopwords\n",
    "sample_tokens = [token for token in sample_tokens if token not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We are down to {len(set(sample_tokens))} unique words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Initial Text Pre-Processing\n",
    "\n",
    "**Activity:** Use what you've learned to preprocess the fourth article. How does the length and number of unique words in the article change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab the fourth document\n",
    "fourth_document = corpus.iloc[3].body\n",
    "fourth_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out how many words it has originally\n",
    "print(f'The 4th document starts off with {len(set(fourth_document.split(\" \")))} unique words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here to process and reduce down to useful words!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing\n",
    "\n",
    "### Stemming\n",
    "Most of the semantic meaning of a word is held in the root, which is usually the beginning of a word.  Conjugations and plurality do not change the semantic meaning. \"eat\", \"eats\", and \"eating\" all have essentially the same meaning. The rest is grammatical variation for the sake of marking things like tense or person or number.   \n",
    "\n",
    "Stemmers consolidate similar words by chopping off the ends of the words.\n",
    "\n",
    "<img src=\"images/stemmer.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different stemmers available.  The two we will use here are the **Porter** and **Snowball** stemmers.  A main difference between the two is how aggressively it stems, Porter being less aggressive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = nltk.stem.PorterStemmer()\n",
    "s_stemmer = nltk.stem.SnowballStemmer(language=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer.stem(sample_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer.stem(sample_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in sample_tokens:\n",
    "    p_word = p_stemmer.stem(word)\n",
    "    s_word = s_stemmer.stem(word)\n",
    "    \n",
    "    if p_word != s_word:\n",
    "        print(word, p_word, s_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = [p_stemmer.stem(word) for word in sample_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(sample_tokens)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Stemming slightly reduced our token count: {len(set(sample_tokens))} unique tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "\n",
    "Lemmatizing is a bit more sophisticated than the stem choppers. Lemmatizing uses part-of-speech tagging to determine how to transform a word.\n",
    "\n",
    "- Unlike Stemming, Lemmatization reduces the inflected words, properly ensuring that the root word belongs to the language. It can handle words such as \"mouse\", whose plural \"mice\" the stemmers would not lump together with the original. \n",
    "\n",
    "- In Lemmatization, the root word is called the \"lemma\". \n",
    "\n",
    "- A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n",
    "\n",
    "<img src=\"images/lemmer.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\"Mice\" becomes: {lemmatizer.lemmatize(\"mice\")}')\n",
    "print(f'\"Media\" becomes: {lemmatizer.lemmatize(sample_tokens[76])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, look at the output below:\n",
    "sentence = \"He saw the trees get sawed down\"\n",
    "lemmed_sentence = [lemmatizer.lemmatize(token) for token in sentence.split(' ')]\n",
    "lemmed_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizers depend, for their full functionality, on POS tagging, and **the default tag is 'noun'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little bit of work, we can POS tag our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "sample_doc = tokenizer.tokenize(sample_document)\n",
    "sample_doc = [token.lower() for token in sample_doc]\n",
    "sample_doc = [token for token in sample_doc if token not in sw]\n",
    "sample_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk's pos_tag to tag our words\n",
    "# Does a pretty good job, but does make some mistakes\n",
    "\n",
    "sample_doc_tagged = pos_tag(sample_doc)\n",
    "sample_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then transform the tags into the tags of our lemmatizers\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_tagged = [(token[0], get_wordnet_pos(token[1]))\n",
    "             for token in sample_doc_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_lemmed = [lemmatizer.lemmatize(token[0], token[1]) for token in sample_doc_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc_lemmed[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(set(sample_doc_lemmed))} unique lemmas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(sample_doc_lemmed)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Text for Modeling\n",
    "\n",
    "The machine learning algorithms we have encountered so far represent features as the variables that take on different value for each observation. For example, we represent individuals with distinct education levels, incomes, and such. However, in NLP, features are represented in a very different way. In order to pass text data to machine learning algorithms and perform classification, we need to represent the features in a sensible way. One such method is called **Bag-of-words (BoW)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling. A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "- A vocabulary of known words.\n",
    "- A measure of the presence of known words.\n",
    "\n",
    "It is called a “bag” of words **because any information about the order or structure of words in the document is discarded**. The model is only concerned with whether known words occur in the document, not with **where** they may occur in the document. The intuition behind BoW is that a document is similar to another if they have similar contents. The Bag of Words method can be represented as a **Document Term Matrix**, in which each column is a unique vocabulary n-gram and each observation is a document. Consider, for example, the following **corpus** of documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document 1: \"I love dogs.\"\n",
    "- Document 2: \"I love cats.\"\n",
    "- Document 3: \"I love all animals.\"\n",
    "- Document 4: \"I hate dogs.\"\n",
    "\n",
    "This corpus can be represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\downarrow$Doc\\|Word$\\rightarrow$|I|love|dogs|cats|all|animals|hate\n",
    "-|-|-|-|-|-|-|-\n",
    "Document_1|1|1|1|0|0|0|0\n",
    "Document_2|1|1|0|1|0|0|0\n",
    "Document_3|1|1|0|0|1|1|0\n",
    "Document_4|1|0|1|0|0|0|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "In order to get these tokens from our documents, we're going to use tools called \"vectorizers\".\n",
    "\n",
    "Documentation! https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CountVectorizer`\n",
    "\n",
    "The most straightforward vectorizer in `sklearn.feature_extraction.text` is the `CountVectorizer`, which will simply count the number of each word type in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "# Note that our Vectorizer wants our text BEFORE tokenizing\n",
    "# So we need to use .join to un-tokenize\n",
    "X = [\" \".join(sample_doc_lemmed)]\n",
    "\n",
    "# Instantiate a CountVectorizer with just default arguments\n",
    "vec = None\n",
    "X_vec = None # Then fit_transform on X\n",
    "\n",
    "df = pd.DataFrame(X_vec.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not very exciting for one document. The idea is to make a document term matrix for all of the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisit our corpus\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our vectorizer can use that same regex pattern to tokenize \n",
    "# while removing punctuation, AND it'll remove stopwords!\n",
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", \n",
    "                      stop_words=sw)\n",
    "# Just exploring for the first 20 documents\n",
    "X_vec = vec.fit_transform(corpus.body[:20])\n",
    "\n",
    "df_tokens = pd.DataFrame(X_vec.toarray(), columns=vec.get_feature_names())\n",
    "df_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can expand the number of ngrams! AKA not single word tokens\n",
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", \n",
    "                      stop_words=sw,\n",
    "                      ngram_range=[1, 2])\n",
    "# Just exploring for the first 20 documents\n",
    "X_vec = vec.fit_transform(corpus.body[:20])\n",
    "\n",
    "df_bigrams = pd.DataFrame(X_vec.toarray(), columns=vec.get_feature_names())\n",
    "df_bigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our document term matrix gets bigger and bigger, with more and more zeros, becoming sparser and sparser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One more time...\n",
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", \n",
    "                      stop_words=sw,\n",
    "                      ngram_range=[1, 2])\n",
    "# But now fit to the entire corpus\n",
    "X_vec = vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X_vec.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we can set upper and lower limits to the word frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                      stop_words=sw, \n",
    "                      ngram_range=[1, 2],\n",
    "                      min_df=2,  # Min # of times a word has to appear\n",
    "                      max_df=100) # Max # of times a word has to appear\n",
    "X_vec = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X_vec.toarray(), columns=vec.get_feature_names())\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TfidfVectorizer`\n",
    "\n",
    "There are many schemas for determining the values of each entry in a document term matrix, and one of the most common uses the TF-IDF algorithm -- \"Term Frequency-Inverse Document Frequency\". Essentially, tf-idf *normalizes* the raw count of the document term matrix. And it represents how important a word is in the given document. \n",
    "\n",
    "> The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\n",
    "\n",
    "**TF (Term Frequency)**: Term frequency is the frequency of the word in the document divided by the total words in the document.\n",
    "\n",
    "**IDF (inverse document frequency)**: Inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is generally calculated as the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "$$\\text{idf}(w) = log (\\frac{\\text{number of documents}}{\\text{num of documents containing} w})$$\n",
    "\n",
    "tf-idf is the product of term frequency and inverse document frequency, or tf * idf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X_vec = tf_vec.fit_transform(corpus.body)\n",
    "\n",
    "df = pd.DataFrame(X_vec.toarray(), columns=tf_vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.iloc[313].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the tfidf to the count vectorizer output for one document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=sw)\n",
    "X_vec = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X_vec.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.iloc[313].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf lessoned the importance of some of the more common words, including a word, \"also\", which might have made it into the stopword list.\n",
    "\n",
    "It also assigns \"nerds\" more weight than power.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\"Nerds\" only shows up in document 313: {len(df_cv[df.nerds!=0])} document.')\n",
    "print(f'\"Power\" shows up in {len(df_cv[df.power!=0])} documents!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the words are stored in a `.vocabulary_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `HashingVectorizer`\n",
    "\n",
    "There is also a hashing vectorizer, which will encrypt all the words of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvec = HashingVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                         stop_words=sw)\n",
    "X_vec = hvec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X_vec.toarray())\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer Summary:\n",
    "\n",
    "Some rules of thumb about these vectorizers:\n",
    "\n",
    "**Tf-Idf**: Probably the most commonly used. Useful when the goal is to distinguish the **content** of documents from others in the corpus.\n",
    "\n",
    "**Count**: Useful when the words themselves matter. If the goal is instead about identifying authors by their words, then the fact that some word appears in many documents of the corpus may be important.\n",
    "\n",
    "**Hashing**: The advantage here is speed and low memory usage. The disadvantage is that you lose the identities of the words being tokenized. Useful for very large datasets where the ultimate model may be a bit of a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "For a final exercise, work through the following:\n",
    "\n",
    "Create a document term matrix of the full 1000-document corpus. \n",
    "\n",
    "The vocabulary should have no stopwords, numbers, or punctuation, and it should be lemmatized. \n",
    "\n",
    "Use a `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
