{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Objectives\n",
    "\n",
    "- Understand what is \"big data\"\n",
    "- Know why big data is different and how it can be processed\n",
    "- Understand how data can be handled in distributed and parallel systems\n",
    "- Understand how MapReduce is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## What is Big Data?\n",
    "\n",
    "There is no clear/agreed upon definition but typically we say we're working on **big data** if we have to use something like a distributed computing system (not just one local machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The 3 Vs of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "+ Volume --> Large Amounts\n",
    "+ Velocity --> Quickly Generated\n",
    "+ Variety --> Unstructured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/3vs.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data is *big* when it is better/faster to split the work over the network amongst more (parallel) because of one or more of these Vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Applying it via Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Hadoop Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/hadoop_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Considered \"old-school\"\n",
    ">\n",
    "> Slower since it has to write to disk each time\n",
    "\n",
    "- Storage (usually HDFS) \n",
    "- Data Processing (MapReduce)\n",
    "- Resource Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/apache_spark_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Holds data in memory whenever possible (faster)\n",
    ">\n",
    "> Can still be built on top of Hadoop but also S3 on AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spark has become king of data since it does a good job with ETL (Extract-Transform-Load) & ML in distributed systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### _Aside: More Detail on Spark_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Some Resources**\n",
    "\n",
    ">[Here](https://towardsdatascience.com/a-neanderthals-guide-to-apache-spark-in-python-9ef1f156d427) is a great walkthrough of Spark basics!\n",
    ">\n",
    "> And [here](https://towardsdatascience.com/apache-spark-a-conceptual-orientation-e326f8c57a64)'s another from our very own alum, Alex Shropshire!\n",
    ">\n",
    "> Spark has APIs for Scala (this is ur-Spark), Java, Python, and R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "N.B. Unless otherwise marked, page references are to [Salloum, Dautov, et al., \"Big Data Analytics on Apache Spark\", 2016](https://link.springer.com/content/pdf/10.1007%2Fs41060-016-0027-9.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Spark is a tool for the management of big data. Sometimes data science professionals will refer to the [five \"V\"s](https://www.bbva.com/en/five-vs-big-data/) of big data. Clearly, the availabilty and size of datasets are growing rapidly. What counts as \"big data\"? Roughly speaking, we're talking about datasets that are too large to be processed on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many large companies are relying on big data these days, and Spark is a major player in the big data game. Examples can be found [here](https://www.icas.com/thought-leadership/technology/10-companies-using-big-data) and [here](https://enlyft.com/tech/products/apache-spark) and [here](https://www.quora.com/Which-are-the-companies-that-use-apache-spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So ... how in the world *do* you process a dataset that's too large for a single machine? You use multiple machines linked together! Let's call each machine a *node*, and the group of all machines working in parallel a *cluster*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The origin story of Spark starts with [MapReduce](https://en.wikipedia.org/wiki/MapReduce), whose programs comprise (unsurprisingly) a \"map\" routine (for filtering and sorting) and a \"reduce\" routine (for performing some aggregate operation).\n",
    "\n",
    "Let's look at an [example](https://en.wikipedia.org/wiki/MapReduce#Logical_view):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An early major player in big data that used MapReduce was [Hadoop](https://en.wikipedia.org/wiki/Apache_Hadoop). Hadoop was (and still is) a framework for distributed data processing. Its processing component used MapReduce, but it also had a storage component called the \"Hadoop Distributed File System\".\n",
    "\n",
    "From Wikipedia: \"Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel. This approach takes advantage of data locality, where nodes manipulate the data they have access to. This allows the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But Spark appeared as open source in 2010, and had some advantages over Hadoop MapReduce.\n",
    "\n",
    "Spark's advances over Hadoop MapReduce:\n",
    "\n",
    "- data processing in memory rather than on disks\n",
    "- a single framework for machine learning, graph analysis, and processing of streaming data (pp. 159-160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For more on the advantages of Spark over MapReduce, see [this piece](https://research.ijcaonline.org/volume113/number1/pxc3900531.pdf).\n",
    "\n",
    "Distributed computing can help enormously with speed. Check out [this website](http://sortbenchmark.org) for the latest in speed records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\"As a framework, it combines a core engine for distributed computing with an advanced programming model for in-memory processing. Although it has the same linear scalability and fault tolerance capabilities as those of MapReduce, it comes with a multistage in-memory programming model comparing to the rigid map-then-reduce disk-based model\" (146).\n",
    "\n",
    "Illustration, p. 148, of Spark guts.\n",
    "\n",
    "\"Running a Spark application involves five key entities ... a driver program, a cluster manager, workers, executors and tasks. A driver program is an application that uses Spark as a library and defines a high-level control flow of the target computation. While a worker provides CPU, memory and storage resources to a Spark application, an executer \\[sic\\] is a JVM (Java Virtual Machine) process that Spark creates on each worker for that application. A job is a set of computations (e.g., a data processing algorithm) that Spark performs on a cluster to get results to the driver program. A Spark application can launch multiple jobs. Spark splits a job into a directed acyclic graph (DAG) of stages where each stage is a collection of tasks. A task is the smallest unit of work that Spark sends to an executor. The main entry point for Spark functionalities is a SparkContext through which the driver program access \\[sic\\] Spark. A SparkContext represents a connection to a computing cluster\" (149)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "RDDs, Transformations, and Actions:\n",
    "\n",
    "Fault tolerance achieved by keeping a record of the RDD's lineage. There are *redundancies* in the data records, so that, in the event of node failure, the other nodes can provide for data recovery. This is what makes these RDDs *resilient*.\n",
    "\n",
    "- Transformations take one from an RDD to another RDD;\n",
    "- Actions take one from an RDD to an output value.\n",
    "\n",
    "Broadcast variables and accumulators act as global variables; the latter are for counters or sums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Surveys of Big Data tools [here](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0032-1) and [here](https://ieeexplore.ieee.org/document/7300948).\n",
    "\n",
    "Debugging can be a challenge in Spark. [This project](https://sites.google.com/site/sparkbigdebug/) was started to help with that.\n",
    "\n",
    "Also check out Paco Nathan's [massive slide show presentation](http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf) on Spark. Let's just look at slides 66-7 and 82."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Spark Data Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***In Pyspark there are only RDD and DataFrames***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In other languages where \"compiling\" is done, there is the distinction between DataFrames and DataSet. \n",
    "\n",
    "![dataframe image](https://databricks.com/wp-content/uploads/2018/05/DataFrames.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Use an RDD when:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)\n",
    "\n",
    "> - you want low-level transformation and actions and control on your dataset;\n",
    "> - your data is unstructured, such as media streams or streams of text;\n",
    "> - you want to manipulate your data with functional programming constructs than domain specific expressions;\n",
    "> - you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Use a dataframe when:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[also quoted from databricks](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> - you want rich semantics, high-level abstractions, and domain specific APIs, use DataFrame\n",
    "> - your processing demands high-level expressions, filters, maps, aggregation, averages, sum, SQL queries, columnar access and use of lambda functions on semi-structured data, use DataFrame\n",
    "> - you want higher degree of type-safety at compile time, want typed JVM objects, take advantage of Catalyst optimization, and benefit from Tungsten’s efficient code generation, use Dataset.\n",
    "> - you want unification and simplification of APIs across Spark Libraries, use DataFrame or Dataset.\n",
    "> - If you are a R user, use DataFrames.\n",
    "> - If you are a Python user, use DataFrames and resort back to RDDs if you need more control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note**: Machine learning algorithms are run on _DataFrames_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## But Spark Isn't Always the Best Tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/tech_stack.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What Do We Mean by \"Parallel\" & \"Distributed\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/types_of_network.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> tasks split up and executed by different workers\n",
    "\n",
    "+ Multiple CPUs each have their own memory\n",
    "+ Multiple CPUs share via a network (using \"messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/sequential.png\" width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Take a step at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/parallel.png\" width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> executing tasks in a non-sequential order\n",
    "\n",
    "+ Multiple CPUs share same memory to \"communicate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Describes two jobs: **Map** & **Reduce**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Software best for **clusters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Below explain each step of the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/MapReduceZooExample.drawio.png\" width-700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Steps in MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](images/mapreduce_visual.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Assign tasks to each worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Map is another word for function: takes in data as one form, and \n",
    "transforms/maps it to another form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We create key-value pairs (tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Reorganize to make reducing easier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Takes data from the map and _combines_ the data into smaller sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark: Programming:\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/flatiron-school/ds-spark/blob/main/spark-programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark: Machine Learning:\n",
    "<a href=\"https://colab.research.google.com/github/flatiron-school/ds-spark/blob/main/spark-ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
