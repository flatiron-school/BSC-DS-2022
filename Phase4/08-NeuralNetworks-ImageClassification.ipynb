{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- use `keras` to code up a neural network model to classify images\n",
    "- explore regularization in neural networks, including adding dropout\n",
    "- recognize the structure of convolutional neural networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_sample_images\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_results(history):\n",
    "    '''\n",
    "    From https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "    \n",
    "    Input: keras history object (output from trained model)\n",
    "    '''\n",
    "    fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
    "    fig.suptitle('Model Results')\n",
    "\n",
    "    # summarize history for accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend(['train', 'test'], loc='upper left')\n",
    "    # summarize history for loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend(['train', 'test'], loc='upper left')\n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "\n",
    "The classic MNIST digits data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing our data directly from keras\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore an example - can adjust the index number to explore\n",
    "sample_index = 42\n",
    "\n",
    "sample_image = X_train[sample_index]\n",
    "sample_label = y_train[sample_index]\n",
    "display(plt.imshow(sample_image))\n",
    "print('Label: {}'.format(sample_label));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to know:\n",
    "\n",
    "- The data and labels in `fit()` need to be numpy arrays, not `pandas` dfs.\n",
    "- Scaling your data will have a large impact on your model.\n",
    "   > For our traditional input features, we would use a scaler object. For images, as long as the minimum value is 0, we can simply divide through by the maximum pixel intensity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize our inputs - from https://keras.io/examples/vision/mnist_convnet/\n",
    "input_shape = (28, 28, 1) # creating this tuple for later\n",
    "\n",
    "# Scale images to the [0, 1] range - 255 is the max intensity\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1) - explicitly say just 1 layer\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")\n",
    "# NOTE don't re-run this cell! Make sure the dimensions are (60000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare our outputs\n",
    "num_classes = 10\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First S*** Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building in a single list - not with add!\n",
    "# Note: it doesn't actually matter how you do this, both are fine\n",
    "model = Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Flatten(), # need to flatten our images to be one long array\n",
    "        layers.Dense(64, activation=\"tanh\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling our model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defining our training hyperparameters\n",
    "# Initial values from: https://keras.io/examples/vision/mnist_convnet/\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "# Fit our model (save output to a history variable)\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs, \n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate!\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two plots above both relating to the quality of our model.  The bottom plot is our loss. It uses the probabilities associated with our predictions to judge how well our prediction fits reality. We want it to decrease as far as possible.\n",
    "\n",
    "The accuracy judges how well the predictions are after applying the threshold at the output layer.  We want accuracy to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: \n",
    "\n",
    "How'd we do?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "Any issues?\n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can adjust if we like! Anyone want to add layers, etc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does regularization make sense in the context of neural networks? <br/>\n",
    "\n",
    "Yes! We still have all of the salient ingredients: a loss function, overfitting vs. underfitting, and coefficients (weights) that could get too large.\n",
    "\n",
    "But there are now a few different flavors besides L1 and L2 regularization. (Note that L1 regularization is not common in the context of  neural networks.)\n",
    "\n",
    "We'll add a few more layers to give regularization a better chance of making a difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste your model here\n",
    "# Then, add kernel_regularizer arguments into the second dense layer!\n",
    "# Might want to add a fewer smaller layers - lets the reg params have impact\n",
    "model = None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "![drop_out](images/drop_out.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. You can add **dropout layers** in your neural network.\n",
    "\n",
    "\n",
    "<img src='images/thanos.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "To add dropout to a `keras` network, simply add it as though it were a layer. It will apply to the immediately preceding layer.\n",
    "\n",
    "Useful resource: see how they add in the Dropout layer! https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "\n",
    "Add Dropout to one or more of your the layers from our base model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paste our base model (without regularization)\n",
    "# Then add dropout!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network is a neural network with **convolutional layers**. CNNs are mainly used for image recognition/classification. They can be used for video analysis, NLP (sentiment analysis, topic modeling), and speech recognition. \n",
    "\n",
    "### How do our brains see an image? \n",
    "\n",
    "We might see some fluffy tail, a wet nose, flappy ears, and a good boy and conclude we are probably seeing a dog. There is not one singular thing about a dog that our brain recognizes as a dog but an amalgamation of different patterns that allow us to make a probable guess.  \n",
    "\n",
    "<img src='images/chihuahua.jpeg'/>\n",
    "\n",
    "### How do computers see images?\n",
    "\n",
    "<img src='images/architecture.jpeg' width=700/>\n",
    "\n",
    "To computers, color images are a 3D object - composed of 3 matrices - one for each primary color that can be combined in varying intensities to create different colors. Each element in a matrix represents the location of a pixel and contains a number between 0 and 255 which indicates the intensity of the corresponding primary color in that pixel.\n",
    "\n",
    "<img src='images/rgb_layers.png'/>\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "**To *convolve* means to roll together**. CNNs make use of linear algebra to identify patterns using the pixel values (intensity of R,G, or B). By **taking a small matrix and moving it across an image and multiplying them together every time it moves**, our network can mathematically identify patterns in these images. This small matrix is known as a *kernel* or *filter* and each one is designed to identify a particular pattern in an image (edges, shapes, etc.)\n",
    "\n",
    "<img src='images/convolve.gif' width=500/>\n",
    "\n",
    "When a filter is \"rolled over\" an image, the resulting matrix is called a **feature map** - literally a map of where each pattern of feature is in the image. Elements with higher values indicate the presence of a pattern the filter is looking for. The values (or weights) of the filter are adjusted during back-propagation.\n",
    "\n",
    "#### Convolutional layer parameters\n",
    "\n",
    "1. Padding: sometimes it is convenient to pad the input volume with zeros around the border. Helps with detecting patterns at the edge of an image\n",
    "2. Stride: the number of pixels to shift the filter on each \"roll\". The larger the stride, the smaller the feature map will be - but we will lose more information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "After a convolutional layer, the feature maps are fed into a max pool layer. Like convolutions, this method is applied one patch at a time (usually 2x2). Max pooling simply takes the largest value from one patch of an image, places it in a new matrix next to the max values from other patches, and discards the rest of the information contained in the activation maps. Other methods exist such as average pooling (taking an average of the patch).\n",
    "\n",
    "<img src='images/maxpool.png'/>\n",
    "\n",
    "This process results in a new feature map with reduced dimensionality that is then passed into another convolution layer to continue the pattern finding process. These steps are repeated until they are passed to a fully connected layer that proceeds to classify the image using the identified patterns.\n",
    "\n",
    "### Flattening\n",
    "\n",
    "Once the neural network has collected a series of patterns that an image contains, it is ready to make a guess as to what the image is. In order to do so, it starts by **flattening** the 2D matrix into a 1D vector, so it can be passed into a normal densely connected layer for classification. Then using this vector, one or many densely connected layers will make a prediction as to what the image is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset!\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "plt.show()\n",
    "# If you get a \"downloading data\" thing, worry not, shouldn't take long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the class balance of our data\n",
    "import pandas as pd\n",
    "pd.DataFrame(y_train)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still need to prep our data\n",
    "# Scale images to the [0, 1] range\n",
    "X_train = X_train.astype(\"float32\") / 255\n",
    "X_test = X_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (32, 32, 3)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(X_train.shape[0], \"train samples\")\n",
    "print(X_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And, prep our outputs\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building using \"add\"\n",
    "\n",
    "cnn = Sequential()\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "cnn.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# now, to get the proper output\n",
    "cnn.add(layers.Flatten())\n",
    "cnn.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "cnn.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "cnn.compile(loss='categorical_crossentropy',\n",
    "            optimizer=\"adam\",\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(X_train,\n",
    "                  y_train,\n",
    "                  epochs=5, # small since this is just a demo\n",
    "                  batch_size=64,\n",
    "                  validation_data=(X_test, y_test))\n",
    "# note - even with just 5 epochs this'll take at least 2.5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate!\n",
    "score = cnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# Visualize results\n",
    "visualize_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A worked-through example on this dataset: https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "intro-to-keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
