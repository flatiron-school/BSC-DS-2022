{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dense](images/dogcat.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Objectives\n",
    "- Describe the basic structure of densely connected neural networks\n",
    "- Describe the concept of backpropagation\n",
    "- Explain the use of gradient descent in neural networks\n",
    "- Use `keras` to code up a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Neural networks have been around for a while. They are over 70 years old, dating back to  their proposal in 1944 by Warren McCullough and Walter Pitts. These first proposed neural nets had thresholds and weights, but no layers and no specific training mechanisms.\n",
    "\n",
    "The \"perceptron\", the first trainable neural network, was created by Frank Rosenblatt in 1957. It consisted of a single layer with adjustable weights in the middle of input and output layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![perceptron](images/nn-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Wait, Wait, Wait... Why a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You really should take a second to realize what tools we already have and ask yourself, \"Do we really need to use this 'neural network' if we already have so many machine learning algorithms?\"\n",
    "\n",
    "And in short, we don't need to default to a neural network but they have advantages in solving very complex problems. It might help to know that idea of neural networks was developed back in the 1950s (perceptron network). It wasn't until we had a lot of data and computational power where they became reasonably useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Talk About Interpretability...\n",
    "\n",
    "![](images/accuracy_vs_interpretability.png)\n",
    "\n",
    "[Image Source](https://medium.com/ansaro-blog/interpreting-machine-learning-models-1234d735d6c9g/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And yet, the pull of deep learning is strong...\n",
    "\n",
    "<img src='images/move_on.jpg' width=350/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Neural Networks\n",
    "\n",
    "- Clustering\n",
    "- Pattern Recognition\n",
    "- Image Recognition (CNN)\n",
    "- Time Series Forecasting (RNN)\n",
    "- Audio/Video/Image Generation (GAN) \n",
    "\n",
    "#### Limitations\n",
    "- Good for prediction, bad for inference \n",
    "- Computationally expensive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Starting with a Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### A Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src='https://cdn-images-1.medium.com/max/1600/0*No3vRruq7Dd4sxdn.png' width=40%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice the similarity to a linear regression:\n",
    "\n",
    "\n",
    "$$ x_1 w_1 + x_2 w_2  + x_3 w_3 = \\text{output}$$\n",
    "$$ XW = \\text{output}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression as a Perceptron\n",
    "\n",
    "* This is **one row of data**, each input is a different feature\n",
    "* Weights are determined through gradient descent \n",
    "* The **bias** term is our logistic regression intercept term\n",
    "* The **activation function** is the sigmoid function that forces output values between 0 and 1\n",
    "* Output is our classification result\n",
    "\n",
    "![](https://miro.medium.com/max/1280/1*8VSBCaqL2XeSCZQe_BAyVA.jpeg)\n",
    "\n",
    "\n",
    "* The perceptron algorithm is about learning the weights for inputs in order to draw a **linear decision boundary** that allows us to discriminate between two linearly separable classes\n",
    "* A perceptron takes in inputs, sums them up with weights, adds a bias, applies some activation function --> output\n",
    "* You can have different activation functions (sigmoid, tanh, ReLu, etc.)\n",
    "* Many perceptrons put together create a neural network\n",
    "\n",
    "<img src='images/perceptron_binary.png'/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Basic Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For our DS purposes, we'll generally imagine our network to consist of only a few layers, including an input layer (where we feed in our data) an output layer (comprising our predictions). Significantly, there will also (generally) be one or more layers of neurons between input and output, called **hidden layers**.\n",
    "\n",
    "One reason these are named hidden layers is that what their output actually represents is _not really known_.  The activation of node 1 of the first hidden layer may represent a sequence of pixel intensity corresponding to a horizontal line, or a group of dark pixels in the middle of a number's loop... etc etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![dense](images/Deeper_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Because we are unaware of how exactly these hidden layers are operating, neural networks are considered **black box** algorithms.  You will not be able to gain much inferential insight from a neural net.\n",
    "\n",
    "Each of our pixels from our digit representation goes to each of our nodes, and each node has a set of weights and a bias term associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Inspiration from Actual Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The composition of neural networks can be **loosely** compared to a neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![neuron](images/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Neural networks draw their inspiration from the biology of our own brains, which are of course also accurately described as 'neural networks'. A human brain contains around $10^{11}$ neurons, connected very **densely**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is a loose analogy, but can be a helpful **mnemonic**. The inputs to our node are like inputs to our neurons. They are either direct sensory information (our features) or input from other axons (nodes passing information to other nodes). The body of our neuron (soma) is where the signals of the dendrites are summed together, which is loosely analogous to our **collector function**. If the summed signal is large enough (our **activation function**), they trigger an action potential which travels down the axon to be passed as output to other dendrites. See [here](https://en.wikipedia.org/wiki/Neuron) for more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Parts of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Layers\n",
    "\n",
    "- **Input Layer**: the initial parameters (these will be the parts we feed to our network)\n",
    "- **Output Layer**: the classification (or regression predictions)\n",
    "- **Hidden Layer(s)**: the other neurons potentially in a neural network to find more complex patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Weights\n",
    "\n",
    "The weights from our inputs are describing how much they should contribute to the next neuron.\n",
    "\n",
    "But we can also think of the weights of hidden layer neurons telling us how much of these linear separations should be combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Activation Functions\n",
    "\n",
    "<img src='images/activation.png' width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The activation function converts our summed inputs into an output, which is then passed on to other nodes in hidden layers, or as an end product in the output layer. This can loosely be thought of as the action potential traveling down the axon.\n",
    "\n",
    "When we build our models in `keras`, we will specify the activation function of both hidden layers and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Other Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll talk more about this when we dive into how to optimize our neural networks, but some hyperparameters include:\n",
    "\n",
    "- **Learning Rate ($\\alpha$)**: how big of a step we take in gradient descent\n",
    "- **Number of Epochs**: how many times we repeat this process\n",
    "- **Batch Size**: how many data points we use in a single training session (1 epoch)\n",
    "    - KEY! This is how often we send results back to update our weights, aka _back-propogation_!\n",
    "\n",
    "Remember, any parameter adjusted to enhance the neural network's learning _is_ a hyperparameter (this includes the actual structure of the neural net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Let's see it in action!\n",
    "\n",
    "Now we know the vocabulary of the different parts, let's try it out for ourselves!\n",
    "\n",
    "First up:\n",
    "- [playground.tensorflow.org](https://playground.tensorflow.org): A visual playground for us to train a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spaceship Titanic Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', )\n",
    "holdout = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='Transported')\n",
    "y = df['Transported']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still need a Train/Test Split\n",
    "\n",
    "I'll note, it's much more often that you'll see train/val/test with neural networks, aka 3 pieces instead of just two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train-test split causing two cols to be bool instead of obj\n",
    "# fixing to pre-empt a type error \n",
    "map_bools = {True: 'True', False:'False', np.nan:np.nan}\n",
    "for col in ['VIP', 'CryoSleep']:\n",
    "    for dataset in [X_train, X_test, holdout]:\n",
    "        dataset[col] = dataset[col].map(map_bools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just going to run our test on num_cols\n",
    "num_cols = [col for col in X_train.columns if X_train[col].dtype != 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[num_cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[num_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use two different imputation strategies\n",
    "median_num_col = ['Age']\n",
    "\n",
    "zero_num_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our imputer\n",
    "median_imp = SimpleImputer(strategy=\"median\")\n",
    "zero_imp = SimpleImputer(strategy='constant', fill_value = 0)\n",
    "\n",
    "imputer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"med\", median_imp, median_num_col),\n",
    "        (\"zero\", zero_imp, zero_num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression to Compare To!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[\n",
    "    ('imputer', imputer),\n",
    "    ('scaler', MinMaxScaler()), \n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "clf.fit(X_train[num_cols], y_train)\n",
    "\n",
    "train_preds = clf.predict(X_train[num_cols])\n",
    "test_preds = clf.predict(X_test[num_cols])\n",
    "\n",
    "print(f\"Train accuracy score: {clf.score(X_train[num_cols], y_train)}\")\n",
    "\n",
    "print(f\"Test accuracy score: {clf.score(X_test[num_cols], y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Our First Model!\n",
    "Models in Keras are defined as a sequence of layers. We create a Sequential model and add layers one at a time until we are happy with our network topology. Documentation: https://keras.io/guides/sequential_model/\n",
    "\n",
    "*   Metrics: https://keras.io/api/metrics/\n",
    "*   Optimizers: https://keras.io/api/optimizers/\n",
    "*   Loss: https://keras.io/api/losses/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess our data first\n",
    "# NNs are linear models - we still need to scale!\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', imputer),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "preprocessor.fit(X_train[num_cols])\n",
    "\n",
    "X_tr_pr = preprocessor.transform(X_train[num_cols])\n",
    "X_te_pr = preprocessor.transform(X_test[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need more imports!\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, regularizers\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note! You may also see `keras` as its own separate library, but it's been integrated into tensorflow since TF V2.0 - you should make sure to use `keras` from the tensorflow library!\n",
    ">\n",
    "> [Source, which includes interesting reading on the history of tensorflow and keras](https://pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way to build models in tensorflow/keras is to create an empty base model and then add layers in order - so that's what we'll do!\n",
    "\n",
    "We'll create a NN with an input layer, one hidden layer, and then an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our base, empty Sequential model\n",
    "model = None\n",
    "\n",
    "# Add a dense input layer -- model.add(Dense())\n",
    "# 12 nodes, input_dim = our # of cols, activation = 'relu'\n",
    "\n",
    "\n",
    "# Add another 12 node dense layer with relu - no need for input_dim\n",
    "\n",
    "\n",
    "# output layer - dense layer with 1 node, activation = 'sigmoid'\n",
    "\n",
    "\n",
    "# And then compile our model! -- model.compile()\n",
    "# loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We then fit our model \n",
    "history = model.fit(X_tr_pr,        # Features\n",
    "                    y_train,        # Target\n",
    "                    epochs=100,     # Number of epochs\n",
    "                    verbose=2,      # Verbosity level - Some output\n",
    "                    batch_size=100, # Number of observations per batch\n",
    "                    validation_data=(X_te_pr, y_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss this summary output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss/accuracy histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "training_acc = history.history['accuracy']\n",
    "test_acc = history.history['val_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Visualize loss history\n",
    "ax1.plot(epoch_count, training_loss, 'r--')\n",
    "ax1.plot(epoch_count, test_loss, 'b-')\n",
    "ax1.legend(['Training Loss', 'Test Loss'])\n",
    "\n",
    "# Visualize accuracy  history\n",
    "ax2.plot(epoch_count, training_acc, 'r--')\n",
    "ax2.plot(epoch_count, test_acc, 'b-')\n",
    "ax2.legend(['Training Accuracy', 'Test Accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LogReg's accuracy score: {clf.score(X_test[num_cols], y_test)}\")\n",
    "print(f\"Simple NN's best accuracy score: {max(test_acc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss!\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break It Down\n",
    "\n",
    "Now that we've built our first model and seen it in action, let's discuss some of those pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "First up, let's talk about our loss function.\n",
    "\n",
    "Loss functions are akin to cost functions we were trying to minimize in gradient descent (i.e. RMSE for linear regression, Gini/entropy for trees)\n",
    "\n",
    "1. For regression problems, keras has **mean_squared_error** or **mean_absolute_error** as a loss function, or **mean_squared_logarithmic_error** if your target has potential outliers\n",
    "2. For binary classification: **binary_crossentropy** (what we used above!)\n",
    "3. For multiclass problems: **categorical_crossentropy**\n",
    "\n",
    "[This article summarizes the above, and more.](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in Neural Networks\n",
    "Neural Nets are usually implemented at scale with large sets of data, therefore optimizing for speed becomes a big concern. Gradient descent can take a very **very** long time to run if we use a single training example every time to update the weights and biases. Therefore, we usually use batch-mode:\n",
    "\n",
    "- **Batch**: \n",
    "In batch gradient descent, we pass all of the training examples through the forward propagation stage before using backpropagation to compute the weights and biases\n",
    "\n",
    "- **Epoch**: \n",
    "An epoch is when you're done passing all training examples through the forward propagation\n",
    "\n",
    "\n",
    "We set our epoch and batch sizes when fitting the model - above, we used 100 for each. That means we broke our data down into chunks of 100 training observations - that's *one batch*. When ALL batches have gone through, that's *one epoch*!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of Gradient Descent\n",
    "- Stochastic Gradient Descent \n",
    "\n",
    "    - SGD calculates the error and update the weight after training each observation in the training set. \n",
    "\n",
    "- Batch Gradient Descent\n",
    "\n",
    "    - Batch calculates the error after each example is trained, but only updates the weight after all of the observations have been trained\n",
    "\n",
    "- Mini-Batch Gradient Descent\n",
    "\n",
    "    - Mini-batch is a compromise between batch and SGD - it splits the training examples into mini batches, and calculates the error and update the weight after each iteration of the mini batches are done training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propogation\n",
    "\n",
    "Forward propogation is how data moves through the network, from the initial input layer through any hidden layers to the output layer.\n",
    "\n",
    "On the first pass, when we feed the node values forward through layers, we initialize the weights with *random* values and biases to be zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back Propogation\n",
    "\n",
    "After a certain number of data points have been passed through the model (batch), the weights will be *updated* with an eye toward optimizing our loss function. (Thinking back to biological neurons, this is like revising their activation potentials.) Typically, this is done by using some version of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the Forward & Back Propogation Process\n",
    "\n",
    "![backprop](images/ff-bb.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot more pieces we'll continue to explore, but for now let's copy our model from above and adjust some pieces to see how its performance changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here to iterate!\n",
    "model_2 = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_2 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test loss/accuracy histories\n",
    "training_loss_2 = history_2.history['loss']\n",
    "test_loss_2 = history_2.history['val_loss']\n",
    "\n",
    "training_acc_2 = history_2.history['accuracy']\n",
    "test_acc_2 = history_2.history['val_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss_2) + 1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Visualize loss history\n",
    "ax1.plot(epoch_count, training_loss_2, 'r--')\n",
    "ax1.plot(epoch_count, test_loss_2, 'b-')\n",
    "ax1.legend(['Training Loss', 'Test Loss'])\n",
    "\n",
    "# Visualize accuracy  history\n",
    "ax2.plot(epoch_count, training_acc_2, 'r--')\n",
    "ax2.plot(epoch_count, test_acc_2, 'b-')\n",
    "ax2.legend(['Training Accuracy', 'Test Accuracy']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LogReg's accuracy score: {clf.score(X_test[num_cols], y_test)}\")\n",
    "print(f\"Simple NN's best accuracy score: {max(test_acc)}\")\n",
    "print(f\"Second NN's best accuracy score: {max(test_acc_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- A very basic, visual intro to neural networks: https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/\n",
    "- Great video explanation of backpropagation by 3Blue1Brown (part of a full playlist): [Backpropagation calculus | Deep learning, chapter 4](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)\n",
    "- These are all neural networks! [The Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/)\n",
    "- Tips and tricks from Stanford (CS 230 - Deep Learning): https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks#good-practices\n",
    "\n",
    "#### Deep Learning Courses:\n",
    "\n",
    "* Google's Machine Learning Crash Course (which uses tensorflow): https://developers.google.com/machine-learning/crash-course/\n",
    "\n",
    "* Deep Learning Wizard (which uses pytorch): https://www.deeplearningwizard.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
