{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(2000)\n",
    "\n",
    "from sklearn.datasets import make_blobs, load_iris\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from src.hier_example import *\n",
    "from src.av_link_agglom_clust import centrAggClust as ac\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- describe the algorithms of agglomerative and divisive hierarchical clustering\n",
    "- compare and contrast hierarchical clustering with $k$-means clustering\n",
    "- implement hierarchical clustering with `scipy` and `sklearn`\n",
    "- build and interpret dendrograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition Behind Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scenario\n",
    "\n",
    "You worked on creating market segmentation groups using kmeans on customer shopping records, but you're not sure kmeans was the best method. By the end of this lesson, you want to find the appropriate number of groups and their descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical Agglomerative Clustering\n",
    "Recall $k$-means clustering where the goal is to assign individual observations to a pre-specified number of clusters according to Euclidean distance between the centroid and the observation. Hierarchical clustering sets out to group the most similar two observations together from a bottom-up level. We end up with a tree-like diagram called a **dendrogram**, which allows us to view the clusterings obtained for each possible number of clusters, from 1 to n. It is up to our discretion as data scientists to decide how many clusters we want. \n",
    "\n",
    "![dendro](images/dendogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "One disadvantage of $k$-means clustering is that we have to specify the number of clusters beforehand. The type of hierchical clustering we will learn today is **agglomerative**, or **bottom-up**, such that we do not have to specify the number of clusters beforehand. \n",
    "\n",
    "There is also **top-down** or **divisive** clustering, where one starts with the entire dataset as a single cluster.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does the algorithm work\n",
    "Initially, every observation is its own cluster. As we move up the leaf of the dendrogram, the two most similar observations fuse together, and then the next most similar clusters fuse together etc. until everything fuses together into a big cluster. Where to stop is up to our discretion. \n",
    "\n",
    "![dendro2](images/400_Basic_Dendrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agglomerative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_agglomerative_algorithm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of hierarchical agglomerative clustering \n",
    "\n",
    "The way that distance is measured between clusters is called the model's **linkage**.\n",
    "\n",
    "- Single Linkage \n",
    "    -  Minimum pair-wise distance: for any two clusters, take one observation from each and determine their distance. Do this over and over, until you have identified the overall minimum pair-wise distance. \n",
    "- Complete Linkage\n",
    "    -  Complete linkage may be defined as the furthest (or maximum) distance between two clusters. That is, all possible pairwise distances between elements (one from cluster A and one from B) are evaluated and the largest value is used as the distance between clusters A & B. This is sometimes called complete linkage and is also called furthest neighbor.\n",
    "- Average Linkage\n",
    "    - The distance between clusters is defined as the average distance between the data points in the clusters. \n",
    "- Ward Linkage\n",
    "    -  Ward method finds the pair of clusters that leads to minimum increase in total within-cluster variance after merging at each step.\n",
    "\n",
    "[This article](https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec) describes the pros and cons of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How well does the dendrogram fit the data?\n",
    "\n",
    "One way of computing this is by means of the **cophenetic correlation coefficient**, which, in a word, is a measure of \"how faithfully the dendrogram preserves the pairwise distances between the \\[datapoints\\]\" -- [Wikipedia](https://en.wikipedia.org/wiki/Cophenetic_correlation).\n",
    "\n",
    "The cophenetic correlation coefficient $c$ is given by [ref](https://www.mathworks.com/help/stats/index.html?/access/helpdesk/help/toolbox/stats/cophenet.html=)\n",
    "\n",
    "![c-coef](images/cophenet.png)\n",
    "\n",
    "\n",
    "$x(i, j) = | Xi − Xj |$, the ordinary Euclidean distance between the $i$th and $j$th observations.<br>\n",
    "$t(i, j)$ = the dendrogrammatic distance between the model points $Ti$ and $Tj$. This distance is the height of the node at which these two points are first joined together.<br>\n",
    "\n",
    "Then, letting ${\\bar {x}}$ be the average of the $x(i, j)$, and letting ${\\bar {t}}$ be the average of the $t(i, j)$, the cophenetic correlation coefficient $c$ is given by[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is complicated! [This site](https://people.revoledu.com/kardi/tutorial/Clustering/Online-Hierarchical-Clustering.html) is helpful on how to understand the cophenetic correlation, and, indeed, on hierarchical clustering generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seeing it in action\n",
    "\n",
    "[This post here](https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/) walks through cluster assignment _step_ by _step_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, Y = make_blobs(n_samples=10, n_features=1,\n",
    "                  centers=5, random_state=42)\n",
    "ac(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Meanwhile, we can do it in _**scipy**_ and _**sklearn**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical clustering with `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Let's generate some data and look at an example of\n",
    "# hierarchical agglomerative clustering.\n",
    "# Generate two clusters: a with 100 points, b with 50:\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "a = np.random.multivariate_normal([10, 0], [[3, 1], [1, 4]], size=[100,])\n",
    "b = np.random.multivariate_normal([0, 20], [[3, 1], [1, 4]], size=[50,])\n",
    "X = np.concatenate((a, b))\n",
    "\n",
    "print(X.shape)  # 150 samples with 2 dimensions\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1])\n",
    "ax.set_title(\"Sample data for clustering demo\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# construct dendrogram in scipy\n",
    "\n",
    "Z = linkage(X, 'single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new variable `Z` contains the dendrogram in coded form! Its third column has distances. https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# calculate and construct the dendrogram \n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "ax.set_title('Hierarchical Clustering Dendrogram')\n",
    "ax.set_xlabel('sample index')\n",
    "ax.set_ylabel('distance')\n",
    "# this dendrogram function imported from scipy.cluster.hierarchy\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# trimming and truncating the dendrogram \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "ax.set_xlabel('sample index')\n",
    "ax.set_ylabel('distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp',  # show only the last p merged clusters\n",
    "    p=6,\n",
    "    show_leaf_counts=False, # otherwise numbers in brackets are counts\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=12.,\n",
    "    show_contracted=True    # to get a distribution impression in\n",
    "                            #truncated branches\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical clustering with `sklearn` on Iris\n",
    "\n",
    "**[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)** for AgglomerativeClustering in `sklearn`\n",
    "\n",
    "\n",
    "**[A great example of using Manhattan distance](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html#sphx-glr-auto-examples-cluster-plot-agglomerative-clustering-metrics-py)** with agglomerative clustering in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# in this case, we won't be working with predicting labels,\n",
    "# so we will only use the features (X)\n",
    "X_iris = iris.data\n",
    "# Grabbing the y values to compare!\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X_iris[:, 1], X_iris[:, 3], c=y_iris);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Comparing our clusters with truth. Note that we are fitting our\n",
    "# model to ALL the columns but only plotting two of them!\n",
    "\n",
    "iris_cluster = AgglomerativeClustering(n_clusters=3)\n",
    "iris_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_iris_clust = iris_cluster.fit_predict(X_iris)\n",
    "\n",
    "# plotting two columns\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(X_iris[:, 1], X_iris[:, 3],\n",
    "              c=y_iris)\n",
    "ax[0].set_title(\"Actual Labels\")\n",
    "\n",
    "ax[1].scatter(X_iris[:, 1], X_iris[:, 3],\n",
    "              c=pred_iris_clust)\n",
    "ax[1].set_title(\"Clustered Labels\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating number of clusters / Cut points\n",
    "For hierarchical agglomerative clustering, or clustering in general, it is generally difficult to truly evaluate the results. Therefore, it is up you, the data scientists, to decide.\n",
    "\n",
    "**[Stanford has a good explanation on page 380](https://nlp.stanford.edu/IR-book/pdf/17hier.pdf)** of your options for picking the cut-off. \n",
    "\n",
    "When we are viewing dendrograms for hierarchical agglomerative clustering, we can visually examine where the natural cutoff is, despite it not sounding exactly statistical, or scientific. We might want to interpret the clusters and assign meanings to them depending on domain-specific knowledge and shape of dendrogram. However, we can evaluate the quality of our clusters using measurements such as Sihouette score discussed in the k-means lectures. \n",
    "\n",
    "(note that hierarchical clustering doesn't have an `inertia_` attribute - there is no centroid! - so we do not calculate elbow scores here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation: silhouette score\n",
    "silhouette_score(X_iris, pred_iris_clust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages & Disadvantages of Hierarchical Clustering\n",
    "\n",
    "#### Advantages\n",
    "- Intuitive and easy to implement\n",
    "- More informative than k-means because it takes individual relationship into consideration\n",
    "- Allows us to look at dendrogram and decide number of clusters\n",
    "\n",
    "#### Disadvantages\n",
    "- Very sensitive to outliers\n",
    "- Cannot undo the previous merge, which might lead to problems later on \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise!\n",
    "\n",
    "Using same online retail data from [UCI database](https://archive.ics.uci.edu/ml/datasets/online+retail).\n",
    "\n",
    "Apply hierarchical clustering to this data, and discuss which works better in this instance: $k$-means or hierarchical clustering? Why do you believe that to be the case?\n",
    "\n",
    "Remember: you'll still need to clean and scale the data! (might want to copy some code from the equivalent exercise from the $k$-means lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shopping = pd.read_csv('data/OnlineRetail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopping.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further reading\n",
    "\n",
    "- [from MIT on just hierarchical](http://web.mit.edu/6.S097/www/resources/Hierarchical.pdf)\n",
    "- [from MIT comparing clustering methods](http://www.mit.edu/~9.54/fall14/slides/Class13.pdf)\n",
    "- [fun CMU slides on clustering](http://www.cs.cmu.edu/afs/andrew/course/15/381-f08/www/lectures/clustering.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
